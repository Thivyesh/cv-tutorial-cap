{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Object Detection with YOLO and OpenCV\n",
    "\n",
    "## Overview:\n",
    "\n",
    "This script demonstrates real-time object detection using the You Only Look Once (YOLO) model, DEtection TRansformers (DETR) model and OpenCV. YOLO is a popular deep learning-based object detection algorithm that is known for its speed and accuracy. DETR is an object detection model that directly predicts object bounding boxes and class labels using transformer-based encoder-decoder architecture. OpenCV is a powerful library used for computer vision tasks, including image processing and object detection.\n",
    "\n",
    "## Concepts:\n",
    "\n",
    "### YOLO (You Only Look Once):\n",
    "   - YOLO is a real-time object detection algorithm that detects objects in images or video frames.\n",
    "   - It divides the input image into a grid and predicts bounding boxes and class probabilities for each grid cell simultaneously.\n",
    "   - YOLO can detect multiple objects in a single pass through the neural network, making it extremely fast.\n",
    "\n",
    "### OpenCV:\n",
    "   - OpenCV (Open Source Computer Vision Library) is an open-source computer vision and machine learning software library.\n",
    "   - It provides a wide range of tools and algorithms for image and video processing tasks.\n",
    "   - OpenCV is widely used for tasks such as object detection, facial recognition, and image segmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection:\n",
    "   - Object detection is a computer vision task that involves detecting and locating objects within an image or video frame.\n",
    "   - It differs from image classification, which identifies the main object in an entire image, by providing the precise location of each object along with its class label.\n",
    "   - Object detection algorithms typically use machine learning techniques, such as deep neural networks, to perform this task.\n",
    "\n",
    "### Real-time Object Detection vs Batch Object Detection:\n",
    "   - Real-time object detection refers to the ability to perform object detection on live video streams in real-time, usually at frame rates of at least 30 frames per second (FPS).\n",
    "   - Batch object detection, on the other hand, involves processing a batch of images or video frames offline, without the constraint of real-time processing.\n",
    "   - Real-time object detection is often used in applications such as video surveillance, autonomous driving, and augmented reality, where timely detection of objects is critical.\n",
    "\n",
    "### YOLOv8 Architecture:\n",
    "   - YOLOv8 (You Only Look Once version 8) is an improvement over previous versions of the YOLO algorithm, known for its efficiency and accuracy in object detection tasks.\n",
    "   - YOLOv8 is based on a deep convolutional neural network architecture that divides the input image into a grid of cells and predicts bounding boxes and class probabilities for each cell simultaneously.\n",
    "   - It uses a single neural network to predict multiple bounding boxes and class probabilities for each object in the image, making it extremely fast and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries:\n",
    "  - The script imports necessary libraries including `cv2` for OpenCV, `YOLO` from `ultralytics` for object detection, and `supervision` for annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing YOLO Model:\n",
    "  - The YOLO model is initialized using pre-trained weights (`yolov8s.pt`). These weights are obtained from training on a large dataset and are used to perform object detection.\n",
    "\n",
    "### Initializing Webcam Capture:\n",
    "  - The script initializes webcam capture using OpenCV's `VideoCapture` class. If the webcam cannot be opened, an error message is printed and the script exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize yolo model\n",
    "model = YOLO('yolov8s.pt')\n",
    "#Initialize webcam capture\n",
    "# If windows use:\n",
    "# self.webcam = cv2.VideoCapture(0, cv2.DSHOW)\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) #cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Object Detection Loop:\n",
    "  - The script enters a while loop to continuously capture frames from the webcam and perform object detection on each frame.\n",
    "  - A video is a sequence of frames (images)\n",
    "  - Each frame captured from the webcam is passed through the YOLO model to detect objects.\n",
    "  - Detected objects are annotated with bounding boxes and labels using the `supervision` library.\n",
    "  - Annotated frames are displayed in real-time using OpenCV's `imshow` function.\n",
    "  - The loop continues until the user presses the 'q' key, at which point the webcam is released and OpenCV windows are closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap.isOpened checks if the camera is connected and can capture(open)\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "while True:\n",
    "    #ret is boolean which is True if the camera stream can be read\n",
    "    #frame is the picture captured during streaming\n",
    "    ret, frame =  cap.read()\n",
    "\n",
    "    #If ret is equal to False the loop will break\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?), Exiting ...\")\n",
    "        break\n",
    "    \n",
    "    #predict with the model\n",
    "    results = model(frame)[0]\n",
    "\n",
    "    #pass the results to the supervision class to process results\n",
    "    #Show example of results\n",
    "    detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "    #Define the annotator that will draw the boundingboxes on the image\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
    "        thickness=4\n",
    "    )\n",
    "\n",
    "    #Define the label annotator which will add label to the annotations\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "    # Remove human class\n",
    "    # detections = detections[detections.class_id !=0]\n",
    "\n",
    "    #Extract the labels from the model object to pass into the label annotator\n",
    "    #The labels will be in a dict format:\n",
    "    # {human: 0}\n",
    "    labels = [\n",
    "            f\"{model.model.names[class_name]} {confidence:.2f}\"\n",
    "            for class_name, confidence\n",
    "            in zip(detections.class_id, detections.confidence)\n",
    "        ]\n",
    "\n",
    "    #Annotate (draw) the fram (image) with boundingboxes\n",
    "    # A bounding boxes is defined by coordinates like for example:\n",
    "    # x1, x2, y1, y2\n",
    "    # These are coordinates \n",
    "    annotated_image = bounding_box_annotator.annotate(\n",
    "        scene=frame, detections=detections)\n",
    "    \n",
    "    # Annotate each bounding box with its label\n",
    "    # Each bounding box will have an id which is linked to each cls (class) which is a int\n",
    "    # The int is used to lookup in the labels dict\n",
    "    annotated_image = label_annotator.annotate(\n",
    "        scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "    #Show the frame with opencv\n",
    "    # When pressing  q the popup window with the frame will close\n",
    "    cv2.imshow(\"frame\", annotated_image)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "#disconnect from the camera\n",
    "cap.release()\n",
    "#Close all cv2 opened windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how the code can be structured as a python class\n",
    "- In this class:\n",
    "  - **__init__**: Initializes the needed variables\n",
    "  - **__del__**: Is a function to disconnect from the camera and close all windows opened by cv2.\n",
    "  - **detect_objects**: Creates a loop where:\n",
    "    - It checks if the camera is connected, if not then disconnect\n",
    "    - predicts, and creates annotations with bounding boxes and labels\n",
    "    - Displays the result with cv2, where you close the opened window by pressing \"q\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO  # Import YOLO model from Ultralytics\n",
    "import supervision as sv  # Import the supervision library for annotations\n",
    "\n",
    "class ObjectDetectionWithWebcam:\n",
    "    \"\"\"\n",
    "    This class performs real-time object detection using a webcam and YOLO model.\n",
    "\n",
    "    Attributes:\n",
    "        model (YOLO): YOLO object detection model.\n",
    "        webcam (cv2.VideoCapture): Webcam object for capturing frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_weights: str = 'yolov8s.pt'):\n",
    "        \"\"\"\n",
    "        Initializes the ObjectDetectionWithWebcam class.\n",
    "\n",
    "        Args:\n",
    "            model_weights (str): Path to the YOLO model weights file (default is 'yolov8s.pt').\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_weights)\n",
    "\n",
    "        # If windows use:\n",
    "        self.webcam = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "        \n",
    "        # If Mac use:\n",
    "        #self.webcam = cv2.VideoCapture(0)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            raise RuntimeError(\"Cannot open webcam\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Cleans up resources by releasing the webcam.\n",
    "        \"\"\"\n",
    "        self.webcam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def detect_objects(self):\n",
    "        \"\"\"\n",
    "        Performs real-time object detection using the webcam and displays the annotated frames.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Read frame from webcam\n",
    "            ret, frame = self.webcam.read()\n",
    "\n",
    "            if not ret:\n",
    "                print(\"Can't receive frame (stream end?), Exiting ...\")\n",
    "                break\n",
    "            \n",
    "            # Perform object detection on the frame using the YOLO model\n",
    "            results = self.model(frame)[0]\n",
    "\n",
    "            # Convert YOLO detections to Supervision Detections format\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "\n",
    "            # Create a bounding box annotator with specified thickness\n",
    "            bounding_box_annotator = sv.BoundingBoxAnnotator(\n",
    "                thickness=4\n",
    "            )\n",
    "\n",
    "            # Create a label annotator\n",
    "            label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "            # Filter out detections with class_id not equal to 0 (background class)\n",
    "            detections = detections[detections.class_id != 0]\n",
    "\n",
    "            # Get labels for each detected object\n",
    "            labels = [\n",
    "                f\"{self.model.model.names[class_name]} {confidence:.2f}\"\n",
    "                for class_name, confidence\n",
    "                in zip(detections.class_id, detections.confidence)\n",
    "            ]\n",
    "\n",
    "            # Annotate the frame with bounding boxes\n",
    "            annotated_image = bounding_box_annotator.annotate(\n",
    "                scene=frame, detections=detections)\n",
    "\n",
    "            # Annotate the frame with labels\n",
    "            annotated_image = label_annotator.annotate(\n",
    "                scene=annotated_image, detections=detections, labels=labels)\n",
    "\n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(\"Object Detection\", annotated_image)\n",
    "\n",
    "            # Exit loop if 'q' key is pressed\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize ObjectDetectionWithWebcam class\n",
    "    detector = ObjectDetectionWithWebcam()\n",
    "\n",
    "    # Perform real-time object detection\n",
    "    detector.detect_objects()\n",
    "    detector.__del__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-tutorial-cap-aU5aeIk7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
