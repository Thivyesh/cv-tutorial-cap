{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETR (DEtection TRansformers) Model:\n",
    "   - DETR is a state-of-the-art object detection model that utilizes transformer architecture, originally proposed by Facebook AI.\n",
    "   - Unlike traditional object detection models that rely on anchor boxes and proposal generation, DETR directly predicts object bounding boxes and class labels in a single pass using transformer-based encoder-decoder architecture.\n",
    "   - It has been shown to achieve competitive performance on object detection benchmarks with fewer heuristics and hyperparameters.\n",
    "\n",
    "## YOLOv8 vs. DETR:\n",
    "\n",
    "### YOLOv8:\n",
    "   - YOLOv8 is well-suited for real-time applications where speed and efficiency are crucial, such as video surveillance and object tracking.\n",
    "   - It provides a simpler and faster approach to object detection compared to DETR, making it easier to deploy in resource-constrained environments.\n",
    "\n",
    "### DETR:\n",
    "   - DETR offers a novel approach to object detection using transformer architecture, which allows for end-to-end training and inference.\n",
    "   - It is suitable for applications where precise localization and accurate detection of objects are important, such as autonomous driving and medical imaging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR vs. YOLOv8 Architecture Comparison\n",
    "\n",
    "## DETR Architecture:\n",
    "- **Encoder-Decoder Architecture**: DETR utilizes a transformer-based encoder-decoder architecture.\n",
    "- **Encoder**: The encoder processes the input image using a series of transformer encoder layers to extract high-level features.\n",
    "- **Decoder**: The decoder generates object queries and attends to the encoded image features to predict object bounding boxes and class labels.\n",
    "- **Positional Encoding**: DETR uses positional encoding to provide spatial information to the transformer model.\n",
    "- **Learnable Class Embeddings**: Instead of using predefined anchor boxes, DETR predicts object classes using learnable class embeddings.\n",
    "- **Direct Prediction**: DETR directly predicts object bounding boxes and class labels in a single pass without the need for anchor box generation or non-maximum suppression.\n",
    "\n",
    "## YOLOv8 Architecture:\n",
    "- **Single-stage Object Detector**: YOLOv8 is a single-stage object detection model based on a deep convolutional neural network (CNN).\n",
    "- **Backbone Network**: YOLOv8 typically uses a CNN backbone network such as Darknet or ResNet to extract features from the input image.\n",
    "- **Grid-based Prediction**: YOLOv8 divides the input image into a grid of cells and predicts bounding boxes and class probabilities for each cell.\n",
    "- **Anchor Boxes**: YOLOv8 uses predefined anchor boxes at different scales and aspect ratios to predict object locations and sizes.\n",
    "- **Non-maximum Suppression**: YOLOv8 performs post-processing steps such as non-maximum suppression to remove redundant detections and refine the final set of predicted bounding boxes.\n",
    "- **Efficiency and Speed**: YOLOv8 is known for its efficiency and speed, making it suitable for real-time object detection tasks.\n",
    "\n",
    "## Differences:\n",
    "- **Architecture Type**: DETR uses a transformer-based encoder-decoder architecture, while YOLOv8 uses a single-stage CNN-based architecture.\n",
    "- **Prediction Strategy**: DETR directly predicts object bounding boxes and class labels in a single pass, while YOLOv8 uses anchor boxes and grid-based prediction.\n",
    "- **Handling of Anchor Boxes**: DETR does not rely on predefined anchor boxes, whereas YOLOv8 uses anchor boxes for object localization.\n",
    "- **Performance vs. Speed**: DETR may offer better accuracy and precise localization but may be slower compared to the highly efficient YOLOv8, which sacrifices a bit of precision for speed.\n",
    "\n",
    "In summary, DETR and YOLOv8 represent different approaches to object detection, with DETR focusing on accuracy and direct prediction using transformers, while YOLOv8 prioritizes efficiency and speed using a single-stage CNN architecture with anchor boxes. The choice between the two depends on the specific requirements of the application, balancing accuracy, speed, and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation:\n",
    "\n",
    "### Importing Libraries:\n",
    "  - The script imports necessary libraries including `cv2` for OpenCV, `pipeline` from `transformers` for object detection, and `PIL` for annotations.\n",
    "\n",
    "### **`__init__`** - Initializing YOLO Model:\n",
    "  - The DETR model is initialized using a pipeline and model checkpoint (`facebook/detr-resnet-50`). These weights are obtained from training on a large dataset and are used to perform object detection.\n",
    "\n",
    "### **`__init__`** - Gracefulle releases camera connection:\n",
    "  - The script releases webcam connection using OpenCV's `VideoCapture` class. If the webcam cannot be opened, an error message is printed and the script exits.\n",
    "\n",
    "### **`__del__`** - Initializing Webcam Capture:\n",
    "  - The script initializes webcam capture using OpenCV's `VideoCapture` class and `release` function. \n",
    "  - `cv2.destroyAllWindows()` closes all windows opened by cv2.\n",
    "\n",
    "### **`detect_objects`** - Real-time Object Detection Loop:\n",
    "  - The script enters a while loop to continuously capture frames from the webcam and perform object detection on each frame.\n",
    "  - Each frame captured from the webcam is passed through the pipeline to detect objects.\n",
    "  - Detected objects are annotated with bounding boxes and labels using the `PIL` library and `ImageDraw` class.\n",
    "  - Annotated frames are displayed in real-time using OpenCV's `imshow` function.\n",
    "  - The loop continues until the user presses the 'q' key, at which point the webcam is released and OpenCV windows are closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import pipeline\n",
    "from PIL import ImageDraw, Image\n",
    "import numpy as np\n",
    "\n",
    "class ObjectDetectionWithWebcam:\n",
    "    \"\"\"\n",
    "    This class performs real-time object detection using a webcam and DETR model.\n",
    "\n",
    "    Attributes:\n",
    "        detector: DETR object detection pipeline.\n",
    "        webcam (cv2.VideoCapture): Webcam object for capturing frames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint: str = \"facebook/detr-resnet-50\"):\n",
    "        \"\"\"\n",
    "        Initializes the ObjectDetectionWithWebcam class.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (str): Name or path of the DETR checkpoint (default is \"facebook/detr-resnet-50\").\n",
    "        \"\"\"\n",
    "        self.detector = pipeline(model=checkpoint, task=\"object-detection\")\n",
    "        self.webcam = cv2.VideoCapture(0)\n",
    "\n",
    "        if not self.webcam.isOpened():\n",
    "            raise RuntimeError(\"Cannot open webcam\")\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"\n",
    "        Cleans up resources by releasing the webcam.\n",
    "        \"\"\"\n",
    "        self.webcam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def detect_objects(self):\n",
    "        \"\"\"\n",
    "        Performs real-time object detection using the webcam and displays the annotated frames.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Read frame from webcam\n",
    "            ret, frame = self.webcam.read()\n",
    "\n",
    "            if not ret:\n",
    "                print(\"Can't receive frame (stream end?), Exiting ...\")\n",
    "                break\n",
    "\n",
    "            # Convert frame to RGB format\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "\n",
    "            # Predict objects in the frame\n",
    "            predictions = self.detector(frame)\n",
    "\n",
    "            # Annotate the frame with predicted bounding boxes and labels\n",
    "            draw = ImageDraw.Draw(frame)\n",
    "            for prediction in predictions:\n",
    "                box = prediction[\"box\"]\n",
    "                label = prediction[\"label\"]\n",
    "                score = prediction[\"score\"]\n",
    "\n",
    "                xmin, ymin, xmax, ymax = box.values()\n",
    "                draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "                draw.text((xmin, ymin), f\"{label}: {round(score, 2)}\", fill=\"white\")\n",
    "\n",
    "            # Convert annotated frame back to OpenCV format\n",
    "            frame = np.array(frame)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Display the annotated frame\n",
    "            cv2.imshow(\"Object Detection\", frame)\n",
    "\n",
    "            # Exit loop if 'q' key is pressed\n",
    "            if cv2.waitKey(1) == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize ObjectDetectionWithWebcam class\n",
    "    detector = ObjectDetectionWithWebcam()\n",
    "\n",
    "    # Perform real-time object detection\n",
    "    detector.detect_objects()\n",
    "    detector.__del__()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-tutorial-cap-aU5aeIk7-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
